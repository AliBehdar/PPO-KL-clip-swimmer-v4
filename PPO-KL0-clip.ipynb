{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights\n",
      "Reward of test Episode0: 154.8390749252514\n",
      "Reward of test Episode1: 154.9740183763482\n",
      "Reward of test Episode2: 138.50202601548153\n",
      "Reward of test Episode3: 136.0636765311649\n",
      "Reward of test Episode4: 152.95351443658905\n"
     ]
    }
   ],
   "source": [
    "# 1.1\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Seed and environment setup\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "#1.2\n",
    "# Custom weight initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_features, n_neuron):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=n_features, out_features=n_neuron, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \"\"\"        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(in_features=n_neuron, out_features=n_neuron, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\"\"\"\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(in_features=n_neuron, out_features=2, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(in_features=n_neuron, out_features=2, bias=True),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "       # y = self.linear1(z)\n",
    "        mu = 2 * self.mu(y)\n",
    "        sigma = self.sigma(y) + 1e-5  # Ensure sigma is never zero\n",
    "        dist = Normal(mu, sigma)\n",
    "        return dist\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_features, n_neuron):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=n_features, out_features=n_neuron, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_neuron, out_features=1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# 1.3\n",
    "# Training process\n",
    "train = False\n",
    "total_steps = 1\n",
    "actor_save_path = \"./actor_weights-and-plot/final_weights\"\n",
    "critic_save_path = \"./critic_weights-and-plot/final_weights\"\n",
    "env = gym.make('Swimmer-v4', render_mode=\"human\" if not train else None).unwrapped\n",
    "num_in = env.observation_space.shape[0]\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "reward_history = []\n",
    "# Hyperparameters\n",
    "GAMMA = 0.999\n",
    "EP_MAX = 1000 \n",
    "EP_LEN = 1000 \n",
    "A_LR = 0.0001\n",
    "C_LR = 0.0001\n",
    "BATCH = 512\n",
    "A_UPDATE_STEPS = 0  \n",
    "C_UPDATE_STEPS = 250 \n",
    "METHOD = [dict(name='kl_pen', kl_target=0.1, lam=0.01), dict(name='clip', epsilon=0.01)][0]\n",
    "\n",
    "\n",
    "class PPO(object):\n",
    "    def __init__(self, n_features, n_neuron, actor_learning_rate, critic_learning_rate, max_grad_norm=0.5):\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.actor_old = Actor(n_features, n_neuron)\n",
    "        self.actor = Actor(n_features, n_neuron)\n",
    "        self.critic = Critic(n_features, n_neuron)\n",
    "        self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=self.critic_lr)\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "       \n",
    "    def save_actor(self, path):\n",
    "        torch.save(self.actor.state_dict(), path)\n",
    "\n",
    "    def save_critic(self, path):\n",
    "        torch.save(self.critic.state_dict(), path)\n",
    "\n",
    "    def update(self, s, a, r, log_old, next_s):\n",
    "        if len(s) == 0:\n",
    "            return  # Skip if the buffer is empty\n",
    "\n",
    "        self.actor_old.load_state_dict(self.actor.state_dict())\n",
    "        state = torch.FloatTensor(s)\n",
    "        action = torch.FloatTensor(a)\n",
    "        discounted_r = torch.FloatTensor(r)\n",
    "        next_state = torch.FloatTensor(next_s)\n",
    "\n",
    "        old_action_log_prob = torch.FloatTensor(log_old)\n",
    "        dist_old = self.actor_old(state)\n",
    "\n",
    "        old_action_log_prob = dist_old.log_prob(action).sum(-1, keepdim=True).detach()\n",
    "\n",
    "        target_v = discounted_r.unsqueeze(-1)\n",
    "        advantage = (target_v - self.critic(state)).detach()\n",
    "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8) # Normalize advantage\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                dist = self.actor(state)\n",
    "                new_action_log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "                new_action_prob = torch.exp(new_action_log_prob)\n",
    "                old_action_prob = torch.exp(old_action_log_prob)\n",
    "\n",
    "                self.kl = torch.distributions.kl_divergence( dist_old, dist).mean()\n",
    "                ratio = new_action_prob / old_action_prob\n",
    "                actor_loss = -torch.mean(ratio * advantage - METHOD['lam'] * self.kl)\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                self.actor_optimizer.step()\n",
    "                if self.kl > 4 * METHOD['kl_target']:\n",
    "                    break\n",
    "            if self.kl < METHOD['kl_target'] / 1.5:\n",
    "                METHOD['lam'] /= 2\n",
    "            elif self.kl > METHOD['kl_target'] * 1.5:\n",
    "                METHOD['lam'] *= 2\n",
    "            METHOD['lam'] = np.clip(METHOD['lam'], 1e-4, 100)\n",
    "        else:\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                dist = self.actor(state)\n",
    "                new_action_log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "                ratio = torch.exp(new_action_log_prob - old_action_log_prob)\n",
    "                L1 = ratio * advantage\n",
    "                L2 = torch.clamp(ratio, 1 - METHOD['epsilon'], 1 + METHOD['epsilon']) * advantage\n",
    "                actor_loss = -torch.min(L1, L2).mean()\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            critic_loss = nn.MSELoss()(self.critic(state), target_v)\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            self.critic_optimizer.step()\n",
    "            critic_losses.append(critic_loss.item())\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.FloatTensor(s).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist = self.actor(s)\n",
    "        action = dist.sample().squeeze(0)\n",
    "        action_log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        return action, action_log_prob\n",
    "\n",
    "    def get_v(self, s):\n",
    "        s = torch.FloatTensor(s).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            value = self.critic(s)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Plotting function\n",
    "def plot_training(episode):\n",
    "    sma = np.convolve(reward_history, np.ones(100) / 100, mode='valid')\n",
    "    plt.figure()\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(reward_history, label='Raw Reward', color='#F6CE3B', alpha=1)\n",
    "    plt.plot(sma, label='SMA 100', color='#385DAA')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if episode == EP_MAX:\n",
    "        plt.savefig('./reward_plot.png', format='png', dpi=600, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Actor Loss\")\n",
    "    plt.plot(actor_losses, label='Loss', color='r', alpha=1)\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Actor Loss\")\n",
    "    if episode == EP_MAX:\n",
    "        plt.savefig('./actor_loss_plot.png', format='png', dpi=600, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Critic Loss\")\n",
    "    plt.plot(critic_losses, label='Critic Loss', color='b', alpha=1)\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Critic Loss\")\n",
    "    if episode == EP_MAX:\n",
    "        plt.savefig('./critic_loss_plot.png', format='png', dpi=600, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "ppo = PPO(n_features=num_in, n_neuron=128, actor_learning_rate=A_LR, critic_learning_rate=C_LR)\n",
    "\n",
    "\n",
    "\n",
    "if train:\n",
    "    for ep in range(1,EP_MAX+1):\n",
    "        s, info = env.reset(seed=seed)\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        buffer_log_old = []\n",
    "        buffer_next_state = []\n",
    "        ep_r = 0\n",
    "        ep_step = 0\n",
    "        for t in range(1,EP_LEN+1):\n",
    "            a, a_log_prob = ppo.choose_action(s)\n",
    "            s_, r, terminated, truncated, info = env.step(a.numpy())\n",
    "            done = terminated or truncated\n",
    "            buffer_s.append(s)\n",
    "            buffer_a.append(a.numpy())\n",
    "            buffer_r.append(r)\n",
    "            buffer_log_old.append(a_log_prob.detach().numpy())\n",
    "            buffer_next_state.append(s_)\n",
    "            s = s_\n",
    "            ep_r += r\n",
    "            ep_step += 1\n",
    "            if (t + 1) % BATCH == 0 or t == EP_LEN - 1 or done:\n",
    "                v_s_ = ppo.get_v(s_)\n",
    "                discounted_r = []\n",
    "                for r in buffer_r[::-1]:\n",
    "                    v_s_ = r + GAMMA * v_s_\n",
    "                    discounted_r.append(v_s_.item())\n",
    "                discounted_r.reverse()\n",
    "\n",
    "                bs = np.array(buffer_s)\n",
    "                ba = np.array(buffer_a)\n",
    "                br = np.array(discounted_r)\n",
    "                blog = np.array(buffer_log_old)\n",
    "                br_next_state = np.array(buffer_next_state)\n",
    "\n",
    "                buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                buffer_log_old = []\n",
    "                buffer_next_state = []\n",
    "\n",
    "                ppo.update(bs, ba, br, blog, br_next_state)\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        reward_history.append(ep_r)\n",
    "            #-- based on interval\n",
    "        if ep % 100 == 0:\n",
    "            #ppo.save(save_path + '_' + f'{ep}' + '.pth')\n",
    "            ppo.save_actor(actor_save_path+ '_' + f'{ep}' + '.pth')\n",
    "            ppo.save_critic(critic_save_path+ '_' + f'{ep}' + '.pth')\n",
    "            plot_training(ep)\n",
    "            print('\\n~~~~~~Interval Save: Model saved.\\n')\n",
    "        result = (f\"Episode: {ep}/1000 | \"\n",
    "                  f\"Episode Reward:{ep_r:.2f} | \"\n",
    "                  f\"lam:{METHOD['lam']} \")      \n",
    "        print(result)\n",
    "    env.close()\n",
    "\n",
    "else:\n",
    "    state_dict_actor = torch.load('./actor_weights-and-plot/final_weights_1000.pth')\n",
    "    state_dict_critic = torch.load('./critic_weights-and-plot/final_weights_1000.pth')\n",
    "    ppo.actor.load_state_dict(state_dict_actor)\n",
    "    ppo.critic.load_state_dict(state_dict_critic)\n",
    "    print(\"Loaded weights\")\n",
    "   \n",
    "    for ep in range(5):\n",
    "        s, info = env.reset(seed=seed)\n",
    "        ep_r = 0\n",
    "        for t in range(1,EP_LEN+1):\n",
    "            a, _ = ppo.choose_action(s)\n",
    "            s_, r, terminated, truncated, info = env.step(a.numpy())\n",
    "            s = s_\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                s, info = env.reset()\n",
    "            ep_r+=r\n",
    "        print(f\"Reward of test Episode{ep}:\",ep_r)\n",
    "\n",
    "    env.close()G1ga999Le00012E01B256\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
